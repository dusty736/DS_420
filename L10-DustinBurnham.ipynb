{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 10 Assignment: RNN\n",
    "## Author: Dustin Burnham\n",
    "### Due: September 15th, 2019\n",
    "\n",
    "Your next generation search engine startup was successful in having the ability to search for images based on their content. As a result, the startup received its second round of funding to be able to search news articles based on their topic. As the lead data scientist, you are tasked to build a model that classifies the topic of each article or newswire. \n",
    "\n",
    "For this assignment, you will leverage the RNN_KERAS.ipynb lab in the lesson. You are tasked to use the Keras Reuters newswire topics classification dataset. This dataset contains 11,228 newswires from Reuters, labeled with over 46 topics. Each wire is encoded as a sequence of word indexes. For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\". As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n",
    "\n",
    "1. Read Reuters dataset into training and testing \n",
    "2. Prepare dataset\n",
    "3. Build and compile 3 different models using Keras LTSM ideally improving model at each iteration.\n",
    "4. Describe and explain your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read Reuters dataset into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.keras.datasets.reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data had trouble being loaded, borrowed this off of stack exchange.  Error coming from allow_pickle argument.\n",
    "\n",
    "import numpy as np\n",
    "# save np.load\n",
    "np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "# call load_data with allow_pickle implicitly set to true\n",
    "(x_train, y_train), (x_test, y_test) = data.load_data(num_words = 10000)\n",
    "\n",
    "# restore np.load for future normal usage\n",
    "np.load = np_load_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word of index\n",
    "word_index = data.get_word_index(path=\"reuters_word_index.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_words=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Newswires in Train:  8982\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Newswires in Train: \", np.shape(x_train)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Newswires in Test:  8982\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Newswires in Test: \", np.shape(x_train)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Count: Counter({3: 3159, 4: 1949, 19: 549, 16: 444, 1: 432, 11: 390, 20: 269, 13: 172, 8: 139, 10: 124, 9: 101, 21: 100, 25: 92, 2: 74, 18: 66, 24: 62, 0: 55, 34: 50, 12: 49, 36: 49, 28: 48, 6: 48, 30: 45, 23: 41, 31: 39, 17: 39, 40: 36, 32: 32, 41: 30, 14: 26, 26: 24, 39: 24, 43: 21, 15: 20, 38: 19, 37: 19, 29: 19, 45: 18, 5: 17, 7: 16, 27: 15, 22: 15, 42: 13, 44: 12, 33: 11, 35: 10})\n"
     ]
    }
   ],
   "source": [
    "print('Class Count: {}'.format(Counter(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Labels:  46\n"
     ]
    }
   ],
   "source": [
    "print('Number of Unique Labels: ', len(set(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30979"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first indices are reserved\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_news(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_remove_common(exclude_num, data):\n",
    "    \"\"\"\n",
    "    Inputs a data frame, excludes the top exclude_num words.\n",
    "    \"\"\"\n",
    "    for i in range(len(data)):\n",
    "        data[i] = list(filter(lambda x: x > exclude_num, data[i]))\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_most_common(include_num, data):\n",
    "    \"\"\"\n",
    "    Inputs a data frame, filters for top include_num words.\n",
    "    \"\"\"\n",
    "    for i in range(len(data)):\n",
    "        data[i] = list(filter(lambda x: x < include_num, data[i]))\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Data\n",
    "x_train = filter_remove_common(20, x_train)\n",
    "x_train = filter_most_common(10000, x_train)\n",
    "\n",
    "x_test = filter_remove_common(20, x_test)\n",
    "x_test = filter_most_common(10000, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'as result its december acquisition space co expects earnings per share 1987 15 30 per share up from 70 cts 1986 company pretax net should rise nine 10 from six 1986 rental operation revenues 19 22 from 12 5 cash flow per share this year should be 2 50 three'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out smple sentence\n",
    "decode_news(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set max newswire length\n",
    "max_review_length = 200\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_review_length)\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set targets to be one-hot-encoded\n",
    "y_train_copy = np.copy(y_train)\n",
    "y_test_copy = np.copy(y_test)\n",
    "y_train = keras.utils.to_categorical(y_train, 46)\n",
    "y_test = keras.utils.to_categorical(y_test, 46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build and compile 3 different models using Keras LTSM ideally improving model at each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0910 23:02:33.359760 4521436608 deprecation.py:323] From /Users/dusty/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 200, 32)           320000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 200, 32)           8320      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 46)                1518      \n",
      "=================================================================\n",
      "Total params: 338,158\n",
      "Trainable params: 338,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/3\n",
      "8982/8982 [==============================] - 28s 3ms/sample - loss: 0.2364 - accuracy: 0.9456 - val_loss: 0.0860 - val_accuracy: 0.9783\n",
      "Epoch 2/3\n",
      "8982/8982 [==============================] - 25s 3ms/sample - loss: 0.0791 - accuracy: 0.9783 - val_loss: 0.0751 - val_accuracy: 0.9783\n",
      "Epoch 3/3\n",
      "8982/8982 [==============================] - 25s 3ms/sample - loss: 0.0739 - accuracy: 0.9783 - val_loss: 0.0731 - val_accuracy: 0.9783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a329590f0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct our model\n",
    "# Similar to model from lab\n",
    "embedding_vecor_length = 32\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(num_of_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(keras.layers.LSTM(32, return_sequences = True))\n",
    "model.add(keras.layers.LSTM(32))\n",
    "model.add(keras.layers.Dense(46, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.83%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.83%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performs very well and is neither over or under fit.  The model accuracy caught me off guard, but I investigated and played with the parameters and found teh same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 16)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 200, 16)           2112      \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 16)                2112      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 46)                782       \n",
      "=================================================================\n",
      "Total params: 165,006\n",
      "Trainable params: 165,006\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/3\n",
      "8982/8982 [==============================] - 43s 5ms/sample - loss: 0.1212 - accuracy: 0.9805 - val_loss: 0.1025 - val_accuracy: 0.9838\n",
      "Epoch 2/3\n",
      "8982/8982 [==============================] - 40s 4ms/sample - loss: 0.0887 - accuracy: 0.9845 - val_loss: 0.0885 - val_accuracy: 0.9863\n",
      "Epoch 3/3\n",
      "8982/8982 [==============================] - 78s 9ms/sample - loss: 0.0834 - accuracy: 0.9862 - val_loss: 0.0924 - val_accuracy: 0.9848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a30e8b4e0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct our model\n",
    "# Change LSTM size, embedding_vector_length, and activation function\n",
    "embedding_vecor_length = 16\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(num_of_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(keras.layers.LSTM(16, return_sequences = True))\n",
    "model.add(keras.layers.LSTM(16))\n",
    "model.add(keras.layers.Dense(46, activation='tanh'))\n",
    "\n",
    "# Compile Model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# Fit Model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.48%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.48%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "scores = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performs slightly better than the last.  The changes I implemented were the smaller LMST size, smaller embedded vector length, and the hyperbolic tangent activation function.  Because this model has roughly half the trainable parameters as the prior model, I expected a model that did not perform as well, but was surprised to beat model 1's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 200, 64)           640000    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 200, 32)           12416     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               16500     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 46)                23046     \n",
      "=================================================================\n",
      "Total params: 700,282\n",
      "Trainable params: 700,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/3\n",
      "8982/8982 [==============================] - 47s 5ms/sample - loss: 0.1072 - accuracy: 0.9758 - val_loss: 0.0726 - val_accuracy: 0.9783\n",
      "Epoch 2/3\n",
      "8982/8982 [==============================] - 44s 5ms/sample - loss: 0.0677 - accuracy: 0.9807 - val_loss: 0.0587 - val_accuracy: 0.9842\n",
      "Epoch 3/3\n",
      "8982/8982 [==============================] - 43s 5ms/sample - loss: 0.0554 - accuracy: 0.9855 - val_loss: 0.0521 - val_accuracy: 0.9873\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a37636ef0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct our model\n",
    "# Changes include larger embedded vector length, added dense layer with dropout.\n",
    "embedding_vecor_length = 64\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(num_of_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(keras.layers.LSTM(32, return_sequences = True))\n",
    "model.add(keras.layers.LSTM(32))\n",
    "model.add(keras.layers.Dense(500))\n",
    "model.add(keras.layers.Dropout(0.3, seed = 42))\n",
    "model.add(keras.layers.Dense(46, activation='sigmoid'))\n",
    "\n",
    "# Compile Model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# Fit Model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.73%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.79%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "scores = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model I build I thought would be the most overfit because of the added dense layer, and that proved correct.  The model performs the best, despite being slightly overfit.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all, these models all performed well, with the worst model performing above 97% accuracy.  This seems suspiciously good, but I can't find where a mistake would come from.  My favorite model was model 3.  This model implemented the LMST with the larger embedded vector and an added dense layer with dropout.  I liked this model because of the it implemented the more cutting edge LMST RNN along with the more classic dense layer with dropout to prevent overfitting.  Overall the overfitting, for this model was minor since the model performed 0.08% better on the training set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
